{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a3a52ed-3422-4d81-b837-23d70451d233",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已保存到 CatchedData\\2025\\01\\20250107SHFE_OUTPUT.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from datetime import date\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 上期所交易数据端口\n",
    "SHFE_daily = 'https://www.shfe.com.cn/reports/tradedata/dailyandweeklydata'\n",
    "OUTPUT_DIR = \"CatchedData\"  # 使用相对路径\n",
    "today = date.today().strftime('%Y%m%d')\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, today[:4], today[4:6], today + \"SHFE_OUTPUT.csv\")\n",
    "\n",
    "def page_catch():\n",
    "    \"\"\"爬取网页并返回 BeautifulSoup 对象，使用显式等待。\"\"\"\n",
    "    options = webdriver.EdgeOptions()\n",
    "    options.add_argument(\"--headless=new\") #设置无头模式\n",
    "    try:\n",
    "        app = webdriver.Edge(options=options)\n",
    "        app.get(SHFE_daily)\n",
    "        # 显式等待，最多等待 10 秒，直到表格元素加载完成\n",
    "        WebDriverWait(app, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, 'div.kx_index_table'))\n",
    "        )\n",
    "        soup = BeautifulSoup(app.page_source, 'html.parser')\n",
    "        app.quit() # 使用 quit() 关闭浏览器，释放资源\n",
    "        return soup\n",
    "    except Exception as e:\n",
    "        print(f\"爬取页面出错: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_csv(soup):\n",
    "    \"\"\"从 BeautifulSoup 对象中提取表格数据并保存到 CSV 文件。\"\"\"\n",
    "    if not soup:\n",
    "        return\n",
    "\n",
    "    # 正确创建所有需要的目录\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "    with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "        tables = soup.select('div.kx_index_table')\n",
    "        for table in tables: # 循环处理每个表格\n",
    "            headers = table.find('thead')\n",
    "            if headers:\n",
    "                header_row = [header.text.strip() for header in headers.find_all('th')]\n",
    "                writer.writerow(header_row)\n",
    "\n",
    "            rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')\n",
    "            for row in rows:\n",
    "                csv_row = []\n",
    "                for cell in row.find_all(['td', 'th']):\n",
    "                    csv_row.append(cell.text.strip())\n",
    "                writer.writerow(csv_row)\n",
    "\n",
    "def main():\n",
    "    soup = page_catch()\n",
    "    if soup:\n",
    "        extract_csv(soup)\n",
    "        print(f\"数据已保存到 {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"没有获取到数据\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1399b6-bd54-4531-8c52-6e5d7db5b836",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
